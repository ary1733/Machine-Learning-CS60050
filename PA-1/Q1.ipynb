{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programming Assignment 1 Q1\n",
    "### Decision Tree\n",
    "\n",
    "Group Number : 46\n",
    "\n",
    "Student 1 : Aryan Singh 19CS30007\n",
    "\n",
    "Student 2 : Seemant Guruprasad Achari 19CS30057\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 'Response'\n",
    "df = pd.read_csv('Dataset_C.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df,train_sample=0.5,target_col= LABEL):\n",
    "    all_indexes=[]\n",
    "    df_grouped= df.groupby(target_col)\n",
    "    for x,x_df in df_grouped:\n",
    "        t = x_df.sample(frac=train_sample,).index\n",
    "        all_indexes.append(t)\n",
    "    g = all_indexes[0].values\n",
    "    for k in all_indexes[1:]:\n",
    "        g=np.hstack([g,k.values])\n",
    "        #np.hstack([all_indexes[0].values,all_indexes[1].values])\n",
    "\n",
    "    train_df = df[df.index.isin(g) ]\n",
    "    test_df = df[~df.index.isin(g)]\n",
    "    return train_df, test_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(df, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Driving_License</th>\n",
       "      <th>Region_Code</th>\n",
       "      <th>Previously_Insured</th>\n",
       "      <th>Vehicle_Age</th>\n",
       "      <th>Vehicle_Damage</th>\n",
       "      <th>Annual_Premium</th>\n",
       "      <th>Policy_Sales_Channel</th>\n",
       "      <th>Vintage</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1-2 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>33536</td>\n",
       "      <td>26</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>&gt; 2 Years</td>\n",
       "      <td>Yes</td>\n",
       "      <td>38294</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>28619</td>\n",
       "      <td>152</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>No</td>\n",
       "      <td>27496</td>\n",
       "      <td>152</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt; 1 Year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2630</td>\n",
       "      <td>160</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Gender  Age  Driving_License  Region_Code  Previously_Insured  \\\n",
       "1   2    Male   76                1            3                   0   \n",
       "2   3    Male   47                1           28                   0   \n",
       "3   4    Male   21                1           11                   1   \n",
       "4   5  Female   29                1           41                   1   \n",
       "5   6  Female   24                1           33                   0   \n",
       "\n",
       "  Vehicle_Age Vehicle_Damage  Annual_Premium  Policy_Sales_Channel  Vintage  \\\n",
       "1    1-2 Year             No           33536                    26      183   \n",
       "2   > 2 Years            Yes           38294                    26       27   \n",
       "3    < 1 Year             No           28619                   152      203   \n",
       "4    < 1 Year             No           27496                   152       39   \n",
       "5    < 1 Year            Yes            2630                   160      176   \n",
       "\n",
       "   Response  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  \n",
       "4         0  \n",
       "5         0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Top five rows of the Data\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop('id', axis = 1) # since ID doesn't help in learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute => Gender has 2 unique values.[ Male, Female, ]\n",
      "Attribute => Age has 66 unique values.\n",
      "Attribute => Driving_License has 2 unique values.[ 1, 0, ]\n",
      "Attribute => Region_Code has 53 unique values.\n",
      "Attribute => Previously_Insured has 2 unique values.[ 0, 1, ]\n",
      "Attribute => Vehicle_Age has 3 unique values.[ > 2 Years, 1-2 Year, < 1 Year, ]\n",
      "Attribute => Vehicle_Damage has 2 unique values.[ Yes, No, ]\n",
      "Attribute => Annual_Premium has 36617 unique values.\n",
      "Attribute => Policy_Sales_Channel has 140 unique values.\n",
      "Attribute => Vintage has 290 unique values.\n",
      "Label     => Response has 2 unique values.[ 1, 0, ]\n"
     ]
    }
   ],
   "source": [
    "# Finding the Discrete Valued attributes in the dataset\n",
    "\n",
    "# max unique categories to consider\n",
    "MAX_UNIQUE_CATEGORIES = 10\n",
    "for k in df.columns:\n",
    "    if k == 'id':\n",
    "        continue\n",
    "    if k == LABEL :\n",
    "        print(\"Label     => \", end = \"\")\n",
    "    else:\n",
    "        print(\"Attribute => \", end = \"\")\n",
    "    print(f\"{k} has {len(df[k].unique())} unique values.\", end=\"\")\n",
    "    if(len(df[k].unique()) <= MAX_UNIQUE_CATEGORIES):\n",
    "        print(\"[ \", end = \"\")\n",
    "        for val in df[k].unique():\n",
    "            print(val, end = \", \")\n",
    "        print(\"]\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attr Gender is categorical\n",
      "Attr Age is continuous\n",
      "Attr Driving_License is categorical\n",
      "Attr Region_Code is continuous\n",
      "Attr Previously_Insured is categorical\n",
      "Attr Vehicle_Age is categorical\n",
      "Attr Vehicle_Damage is categorical\n",
      "Attr Annual_Premium is continuous\n",
      "Attr Policy_Sales_Channel is continuous\n",
      "Attr Vintage is continuous\n"
     ]
    }
   ],
   "source": [
    "CATEGORICAL_FEATURE = \"categorical\"\n",
    "CONTINUOUS_FEATURE = \"continuous\"\n",
    "def determineTypeOfFeature(df):\n",
    "    \n",
    "    feature_types = {}\n",
    "    n_unique_values_treshold = MAX_UNIQUE_CATEGORIES\n",
    "    for feature in df.columns:\n",
    "        if feature == 'id':\n",
    "            continue\n",
    "        if feature != LABEL:\n",
    "            unique_values = df[feature].unique()\n",
    "            example_value = unique_values[0]\n",
    "\n",
    "            if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_treshold):\n",
    "                feature_types[feature] = CATEGORICAL_FEATURE\n",
    "            else:\n",
    "                feature_types[feature] = CONTINUOUS_FEATURE\n",
    "    \n",
    "    return feature_types\n",
    "FEATURE_TYPES = determineTypeOfFeature(df)\n",
    "for k, v in FEATURE_TYPES.items():\n",
    "    print(f\"Attr {k} is {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "POTENTIAL_SPLIT_LIMIT = 50\n",
    "def getPotentialSplits(featureName, trainData):\n",
    "    \n",
    "    potential_splits = []\n",
    "    values = trainData[featureName]\n",
    "    unique_values = np.unique(values)\n",
    "    \n",
    "    type_of_feature = FEATURE_TYPES[featureName]\n",
    "    if type_of_feature == CONTINUOUS_FEATURE:\n",
    "        for index in range(len(unique_values)):\n",
    "            if index != 0:\n",
    "                current_value = unique_values[index]\n",
    "                previous_value = unique_values[index - 1]\n",
    "                potential_split = (current_value + previous_value) / 2\n",
    "                \n",
    "                potential_splits.append(potential_split)\n",
    "        \n",
    "        # Since the potential_splits can have thousands of options, we need to drop some of them to reduce the complexity\n",
    "        if(len(potential_splits) > POTENTIAL_SPLIT_LIMIT):\n",
    "            # startIdx = 0\n",
    "            endIdx = len(potential_splits)\n",
    "            incrementBy = (endIdx + POTENTIAL_SPLIT_LIMIT -1)// POTENTIAL_SPLIT_LIMIT\n",
    "            # reducedPotentialList = [potential_splits[idx] for idx in range(startIdx, endIdx, incrementBy)]\n",
    "            potential_splits = potential_splits[::incrementBy]\n",
    "    # feature is categorical\n",
    "    # (there need to be at least 2 unique values, otherwise in the\n",
    "    # split_data function data_below would contain all data points\n",
    "    # and data_above would be empty)\n",
    "    elif len(unique_values) > 1:\n",
    "        potential_splits = unique_values\n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.5,\n",
       " 16.5,\n",
       " 22.5,\n",
       " 28.5,\n",
       " 34.5,\n",
       " 40.5,\n",
       " 46.5,\n",
       " 52.5,\n",
       " 58.5,\n",
       " 64.5,\n",
       " 70.5,\n",
       " 76.5,\n",
       " 82.5,\n",
       " 88.5,\n",
       " 94.5,\n",
       " 100.5,\n",
       " 106.5,\n",
       " 112.5,\n",
       " 118.5,\n",
       " 124.5,\n",
       " 130.5,\n",
       " 136.5,\n",
       " 142.5,\n",
       " 148.5,\n",
       " 154.5,\n",
       " 160.5,\n",
       " 166.5,\n",
       " 172.5,\n",
       " 178.5,\n",
       " 184.5,\n",
       " 190.5,\n",
       " 196.5,\n",
       " 202.5,\n",
       " 208.5,\n",
       " 214.5,\n",
       " 220.5,\n",
       " 226.5,\n",
       " 232.5,\n",
       " 238.5,\n",
       " 244.5,\n",
       " 250.5,\n",
       " 256.5,\n",
       " 262.5,\n",
       " 268.5,\n",
       " 274.5,\n",
       " 280.5,\n",
       " 286.5,\n",
       " 292.5,\n",
       " 298.5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPotentialSplits('Vintage', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeaturesList(trainData):\n",
    "    featuresList = []\n",
    "    for key, value in FEATURE_TYPES.items():\n",
    "        if(value == CONTINUOUS_FEATURE):\n",
    "            potentialSplits = getPotentialSplits(key, trainData)\n",
    "            for potentialSplit in potentialSplits:\n",
    "                featuresList.append([key, potentialSplit])\n",
    "        else:\n",
    "            featuresList.append([key])\n",
    "    \n",
    "    return featuresList  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to Calulate Total Entropy\n",
    "\n",
    "# Input : Traning Data, label, classList\n",
    "\n",
    "def calculateTotalEntropy(trainData, label, classList):\n",
    "    totalCount = trainData.shape[0]\n",
    "\n",
    "    totalEntropy = 0\n",
    "\n",
    "    for c in classList:\n",
    "        # Count the number of data points which satify == c class constraints\n",
    "        totalClassCount = trainData[trainData[label] == c].shape[0]\n",
    "        # Calulate the class's entropy\n",
    "        totalClassEntropy = 0\n",
    "        prob = totalClassCount / totalCount\n",
    "        if( prob >0):\n",
    "            totalClassEntropy = - (prob)*np.log2(prob)\n",
    "        # Accumulate the class's entropy into the totalEntropy\n",
    "        totalEntropy = totalEntropy + totalClassEntropy\n",
    "    \n",
    "    return totalEntropy\n",
    "\n",
    "def calculateEntropy(featureValueData, label, classList):\n",
    "    classCount = featureValueData.shape[0]\n",
    "    entropy = 0\n",
    "\n",
    "    for c in classList:\n",
    "        # Count the number of data points withing the featureValueData which satify == c class constraints\n",
    "        labelClassCount = featureValueData[featureValueData[label] == c].shape[0]\n",
    "        classEntropy = 0\n",
    "        if labelClassCount != 0:\n",
    "            # if labelClassCount != 0 then calculate its entropy\n",
    "            prob = labelClassCount/classCount\n",
    "            if prob !=0:\n",
    "                classEntropy = -prob * np.log2(prob)\n",
    "        # Accumulate its entropy\n",
    "        entropy = entropy + classEntropy\n",
    "    return entropy\n",
    "\n",
    "def calulateInformationGain(feature, trainData, label, classList):\n",
    "    featureName = feature[0]\n",
    "    threshold = None\n",
    "\n",
    "    # featureValueList = getPotentialSplits(featureName, trainData)\n",
    "    # featureValueList = trainData[featureName].unique() # here UPDATE\n",
    "\n",
    "    totalCount = trainData.shape[0]\n",
    "    featureInfo = 0.0\n",
    "        \n",
    "    if(FEATURE_TYPES[featureName] == CONTINUOUS_FEATURE):\n",
    "        # For cases when value <= threshold\n",
    "        threshold = feature[1]\n",
    "        featureValueData = trainData[trainData[featureName] <= threshold]\n",
    "        featureValueCount = featureValueData.shape[0]\n",
    "        featureValueEntropy = calculateEntropy(featureValueData, label, classList)\n",
    "        featureValueProbability = featureValueCount / totalCount\n",
    "        featureInfo += featureValueProbability * featureValueEntropy\n",
    "\n",
    "        # For cases when value > threshold\n",
    "        featureValueData = trainData[trainData[featureName] > threshold]\n",
    "        featureValueCount = featureValueData.shape[0]\n",
    "        featureValueEntropy = calculateEntropy(featureValueData, label, classList)\n",
    "        featureValueProbability = featureValueCount / totalCount\n",
    "        featureInfo += featureValueProbability * featureValueEntropy\n",
    "    else:\n",
    "        featureValueList = trainData[featureName].unique()\n",
    "        for featureValue in featureValueList:\n",
    "            featureValueData = trainData[trainData[featureName] == featureValue]\n",
    "            featureValueCount = featureValueData.shape[0]\n",
    "            featureValueEntropy = calculateEntropy(featureValueData, label, classList)\n",
    "            featureValueProbability = featureValueCount / totalCount\n",
    "            featureInfo += featureValueProbability * featureValueEntropy\n",
    "    \n",
    "    totalEntropy = calculateTotalEntropy(trainData, label, classList)\n",
    "\n",
    "    gain = totalEntropy - featureInfo\n",
    "    \n",
    "    return gain \n",
    "\n",
    "def findMostImportantFeature(trainData, featureList, label, classList):\n",
    "    # featureList = trainData.columns.drop(label) # Since label is not a feature\n",
    "    # featureList = getFeaturesList(trainData) \n",
    "    \n",
    "    maxInfoGain = -1 # init with smallest value possible\n",
    "    maxInfoFeature = None\n",
    "    # features are of two types: \n",
    "    # 1) Categorical, in which you iterate over multiple value of that feature\n",
    "    # 2) Continous converted into binary category, i/p<= threshold or ip>threshold\n",
    "    for feature in featureList:\n",
    "        featureInfoGain = calulateInformationGain(feature, trainData, label, classList)\n",
    "        # print(feature, featureInfoGain, maxInfoGain)\n",
    "        if float(featureInfoGain) > float(maxInfoGain):\n",
    "            maxInfoGain = featureInfoGain\n",
    "            maxInfoFeature = feature\n",
    "    return maxInfoFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entropy of Complete Data Set with Response as label = 0.5362909567366381\n"
     ]
    }
   ],
   "source": [
    "print(f\"The entropy of Complete Data Set with {LABEL} as label = {calculateTotalEntropy(df, LABEL, df[LABEL].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "findMostImportantFeature() missing 1 required positional argument: 'classList'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8128\\1368358824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The max info gain feature is {findMostImportantFeature(df, LABEL, df[LABEL].unique())}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: findMostImportantFeature() missing 1 required positional argument: 'classList'"
     ]
    }
   ],
   "source": [
    "print(f\"The max info gain feature is {findMostImportantFeature(df, LABEL, df[LABEL].unique())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1147977301372255"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calulateInformationGain(['Previously_Insured',298.5], df, LABEL, df[LABEL].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4327494857191425e-07"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calulateInformationGain(['Vintage',150.5], df, LABEL, df[LABEL].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calulateInformationGain(['Age',2900], df, LABEL, df[LABEL].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 131689 entries, 0 to 131688\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count   Dtype \n",
      "---  ------                --------------   ----- \n",
      " 0   id                    131689 non-null  int64 \n",
      " 1   Gender                131689 non-null  object\n",
      " 2   Age                   131689 non-null  int64 \n",
      " 3   Driving_License       131689 non-null  int64 \n",
      " 4   Region_Code           131689 non-null  int64 \n",
      " 5   Previously_Insured    131689 non-null  int64 \n",
      " 6   Vehicle_Age           131689 non-null  object\n",
      " 7   Vehicle_Damage        131689 non-null  object\n",
      " 8   Annual_Premium        131689 non-null  int64 \n",
      " 9   Policy_Sales_Channel  131689 non-null  int64 \n",
      " 10  Vintage               131689 non-null  int64 \n",
      " 11  Response              131689 non-null  int64 \n",
      "dtypes: int64(9), object(3)\n",
      "memory usage: 12.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Gender', 'categorical'), ('Age', 'continuous'), ('Driving_License', 'categorical'), ('Region_Code', 'continuous'), ('Previously_Insured', 'categorical'), ('Vehicle_Age', 'categorical'), ('Vehicle_Damage', 'categorical'), ('Annual_Premium', 'continuous'), ('Policy_Sales_Channel', 'continuous'), ('Vintage', 'continuous')])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURE_TYPES.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [x for x in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 7, 14, 21, 28, 35, 42, 49]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[::7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for decision node and tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  passing same feature list\n",
    "MAX_DEPTH = 3\n",
    "class DecisionNode:\n",
    "    def __init__(self, featureList, label, classList, decisionNodeName, depth):\n",
    "        self.featureList = featureList\n",
    "        self.decisionNodeName = decisionNodeName\n",
    "        self.label = label\n",
    "        self.classList = classList\n",
    "        self.leafNode = False\n",
    "        self.isPruned = False\n",
    "        self.depth = depth\n",
    "        if( self.depth == MAX_DEPTH ):\n",
    "            self.isPruned = True\n",
    "    \n",
    "    def fitData(self, trainData):\n",
    "        label = self.label\n",
    "        classList = self.classList\n",
    "        self.labelDistribution = trainData[label].value_counts()\n",
    "        self.trainDataCount = trainData.shape[0]\n",
    "        self.currentNodeReturnVal = trainData[label].mode()[0]  # return this if you cant decide\n",
    "        self.inpurity = calculateTotalEntropy(trainData, label, classList)\n",
    "\n",
    "        if(self.labelDistribution[0] == 0 or self.labelDistribution[1] == 0 or self.trainDataCount < 50 or self.isPruned):\n",
    "            self.leafNode = True\n",
    "\n",
    "        self.childrenNode = []\n",
    "        if(not self.leafNode):\n",
    "            label = self.label\n",
    "            classList = self.classList\n",
    "            retVal = findMostImportantFeature(trainData, self.featureList, label, classList)\n",
    "            self.bestFeatureName = retVal[0]\n",
    "            print(self.bestFeatureName)\n",
    "            if(FEATURE_TYPES[self.bestFeatureName] == CONTINUOUS_FEATURE):\n",
    "                self.threshold = retVal[1]\n",
    "                # left is <= threshold\n",
    "                nextNodeName = self.bestFeatureName + \" <= \" + str(self.threshold)\n",
    "                print(f\"The next node will go by the name {nextNodeName}\")\n",
    "                node = DecisionNode(self.featureList, self.label, self.classList, nextNodeName, self.depth + 1)\n",
    "                node.fitData(trainData[trainData[self.bestFeatureName] <= self.threshold])\n",
    "                self.childrenNode.append(node)\n",
    "\n",
    "                # right is > threshold\n",
    "                nextNodeName = self.bestFeatureName + \" > \" + str(self.threshold)\n",
    "                print(f\"The next node will go by the name {nextNodeName}\")\n",
    "                node = DecisionNode(self.featureList, self.label, self.classList, nextNodeName, self.depth + 1)\n",
    "                node.fitData(trainData[trainData[self.bestFeatureName] > self.threshold])\n",
    "                self.childrenNode.append(node)\n",
    "\n",
    "            else:\n",
    "                attributeValues = trainData[self.bestFeatureName].unique()\n",
    "                for nextValue in attributeValues:\n",
    "                    nextNodeName = self.bestFeatureName + \" == \" + str(nextValue)\n",
    "                    print(f\"The next node will go by the name {nextNodeName}\")\n",
    "                    node = DecisionNode(self.featureList, self.label, self.classList, nextNodeName, self.depth + 1)\n",
    "                    node.fitData(trainData[trainData[self.bestFeatureName] == nextValue])\n",
    "                    self.childrenNode.append(node)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            self.decisionNodeName = self.decisionNodeName\n",
    "            self.bestFeatureName = \"Ans = \" + str(self.currentNodeReturnVal)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Name: {self.decisionNodeName}\\nData Count: {self.trainDataCount}\\nFeature: {self.bestFeatureName}\\nChildren: {len(self.childrenNode)}\\n\"\n",
    "            \n",
    "    def print_tree(self, op_file):\n",
    "        '''\n",
    "            This function prints the decision tree using the graphviz library.\n",
    "            It basically does a bfs on the graph and then appends the nodes one by one.\n",
    "        '''\n",
    "        \n",
    "        my_graph= Digraph('Decision Tree', filename=op_file)\n",
    "        my_graph.attr(rankdir='LR', size='1000,500')\n",
    "\n",
    "        my_graph.attr('node', shape='rectangle')\n",
    "        \n",
    "        # doing a bfs using a queue\n",
    "        qq = [self]                          # using a list as a queue for the bradth first search\n",
    "        while len(qq) > 0:\n",
    "            node = qq.pop(0)         \n",
    "            if node.leafNode:                    # stop if its a leaf node\n",
    "                continue\n",
    "            for child in node.children.items():    # else check its children\n",
    "                my_graph.edge(str(node), str(child))\n",
    "                qq.append(child)\n",
    "\n",
    "        # my_graph.render(op_file, view=True)     # open the output file for convenience\n",
    "\n",
    "        return      \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureList = getFeaturesList(train_set)\n",
    "root= DecisionNode(featureList, LABEL, train_set[LABEL].unique(), \"Root\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previously_Insured\n",
      "The next node will go by the name Previously_Insured == 0\n",
      "Vehicle_Damage\n",
      "The next node will go by the name Vehicle_Damage == No\n",
      "The next node will go by the name Vehicle_Damage == Yes\n",
      "The next node will go by the name Previously_Insured == 1\n",
      "Vehicle_Damage\n",
      "The next node will go by the name Vehicle_Damage == No\n",
      "The next node will go by the name Vehicle_Damage == Yes\n"
     ]
    }
   ],
   "source": [
    "root.fitData(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DecisionNode' object has no attribute 'children'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20320\\3638189007.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mroot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'FirstTreePic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20320\\423126172.py\u001b[0m in \u001b[0;36mprint_tree\u001b[1;34m(self, op_file)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleafNode\u001b[0m\u001b[1;33m:\u001b[0m                    \u001b[1;31m# stop if its a leaf node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# else check its children\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m                 \u001b[0mmy_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[0mqq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DecisionNode' object has no attribute 'children'"
     ]
    }
   ],
   "source": [
    "root.print_tree('FirstTreePic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(root, op_file):\n",
    "        '''\n",
    "            This function prints the decision tree using the graphviz library.\n",
    "            It basically does a bfs on the graph and then appends the nodes one by one.\n",
    "        '''\n",
    "        \n",
    "        my_graph= Digraph('Decision Tree', filename=op_file)\n",
    "        my_graph.attr(rankdir='LR', size='1000,500')\n",
    "\n",
    "        my_graph.attr('node', shape='rectangle')\n",
    "        \n",
    "        # doing a bfs using a queue\n",
    "        qq = [root]                          # using a list as a queue for the bradth first search\n",
    "        while len(qq) > 0:\n",
    "            node = qq.pop(0)         \n",
    "            if node.leafNode:                    # stop if its a leaf node\n",
    "                continue\n",
    "            for child in node.childrenNode:    # else check its children\n",
    "                my_graph.edge(str(node), str(child))\n",
    "                qq.append(child)\n",
    "\n",
    "        my_graph.render(op_file, view=True)     # open the output file for convenience\n",
    "\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute 'dot', make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Books\\ML\\Assignments\\PA-1\\venv\\lib\\site-packages\\graphviz\\backend\\execute.py\u001b[0m in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\ary17\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\ary17\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\users\\ary17\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8128\\2166838906.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./pic.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# root\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8128\\813154982.py\u001b[0m in \u001b[0;36mprint_tree\u001b[1;34m(root, op_file)\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mqq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mmy_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# open the output file for convenience\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Books\\ML\\Assignments\\PA-1\\venv\\lib\\site-packages\\graphviz\\_tools.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m                               category=category)\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Books\\ML\\Assignments\\PA-1\\venv\\lib\\site-packages\\graphviz\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mrendered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Books\\ML\\Assignments\\PA-1\\venv\\lib\\site-packages\\graphviz\\_tools.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m                               category=category)\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Books\\ML\\Assignments\\PA-1\\venv\\lib\\site-packages\\graphviz\\backend\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[0;32m    325\u001b[0m                       \u001b[0mcwd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparts\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                       \u001b[0mquiet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m                       capture_output=True)\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Books\\ML\\Assignments\\PA-1\\venv\\lib\\site-packages\\graphviz\\backend\\execute.py\u001b[0m in \u001b[0;36mrun_check\u001b[1;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mExecutableNotFound\u001b[0m: failed to execute 'dot', make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "print_tree(root, './pic.png')\n",
    "# root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a300604ec19f1262707987b8931cb13819a0956c707061a8a3861e9aea3c0e0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
